# ğŸ“ KIDZ-GPT: AI Learning Companion for Kids

A vibrant, interactive learning platform that uses AI and 3D character animation to teach children aged 6-10 in multiple languages. KIDZ-GPT combines voice recognition, intelligent tutoring, animated explanations, and interactive quizzes to create an engaging educational experience.

## âœ¨ Features

### ğŸ¤ **Multi-Language Voice Input**
- Speech-to-text using OpenAI Whisper (supports 99+ languages)
- Automatic language detection from audio
- Live transcription with visual feedback
- Supports: English, Hindi, Bengali, Tamil, Telugu, and more

### ğŸ¤– **AI-Powered Responses**
- LLM-driven explanations using Ollama (gpt-oss:120b-cloud)
- Context-aware learning with intent extraction
- Child-friendly explanations with key points
- Automatic Wikipedia image fetching for topics

### ğŸ¨ **3D Character Animation**
- Animated characters (Boy & Girl avatars) with expressions
- Real-time animation synchronized with speech
- Multiple animation actions (hello, thinking, jumping, etc.)
- Scene-based storytelling approach

### ğŸ“š **Interactive Learning**
- **Topic Explanations**: Title, summary, key points + Wikipedia images
- **Quiz Generation**: AI-generated quizzes based on explained topics
- **Scoring System**: Real-time quiz feedback with scoring
- **Colorful Chat Interface**: Vibrant gradient bubbles for user/AI messages

### ğŸ§ **Web Speech API TTS**
- Browser-native text-to-speech
- Multiple voice options per language
- Natural speaking rate and pitch control
- Playback controls (stop, replay)

### ğŸŒ **Multi-Language Support**
- **Full Pipeline**: STT â†’ Intent Extraction â†’ Storyboard â†’ Animation â†’ TTS
- All components respond in the detected/selected language
- Fallback explainers for all supported languages

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KIDZ-GPT System                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Frontend (React + TypeScript + Vite)       â”‚   â”‚
â”‚  â”‚  â”œâ”€ Chat Interface (Colorful Bubbles)             â”‚   â”‚
â”‚  â”‚  â”œâ”€ 3D Scene Player (Three.js + React Three)      â”‚   â”‚
â”‚  â”‚  â”œâ”€ Explainer Section (Images + Content)          â”‚   â”‚
â”‚  â”‚  â”œâ”€ Quiz Interface                                â”‚   â”‚
â”‚  â”‚  â””â”€ Web Speech API (TTS & STT)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†• (HTTP)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Backend (Python FastAPI)                   â”‚   â”‚
â”‚  â”‚  â”œâ”€ /process (Audio Upload)                       â”‚   â”‚
â”‚  â”‚  â”œâ”€ /process-text (Text Input)                    â”‚   â”‚
â”‚  â”‚  â”œâ”€ /generate-quiz (Quiz Creation)                â”‚   â”‚
â”‚  â”‚  â””â”€ /transcribe (Whisper Server)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†•                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   External Services & Local Services              â”‚   â”‚
â”‚  â”‚  â”œâ”€ Ollama (LLM: gpt-oss:120b-cloud)             â”‚   â”‚
â”‚  â”‚  â”œâ”€ OpenAI Whisper (Speech Recognition)          â”‚   â”‚
â”‚  â”‚  â”œâ”€ Wikipedia API (Image Fetching)               â”‚   â”‚
â”‚  â”‚  â”œâ”€ langdetect (Text Language Detection)         â”‚   â”‚
â”‚  â”‚  â””â”€ Redis Cache (Response Caching)               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Internal System Flow

### 1. **Audio Input Processing**
```
User speaks (audio) 
  â†“
Frontend captures via Web Audio API
  â†“
Sends to /process endpoint with language hint
  â†“
Backend STT Service (Whisper)
  â†“
Whisper detects language & transcribes
  â†“
Returns: {text, detected_language}
```

### 2. **Intent Extraction**
```
Transcribed text + language
  â†“
Intent Agent (LLM via Ollama)
  â†“
Extracts: {topic, question_type, difficulty}
  â†“
Result used for storyboard generation
```

### 3. **Storyboard Generation**
```
Intent + question + language
  â†“
Script Agent (LLM via Ollama)
  â†“
Generates educational scenes with dialogues
  â†“
Each scene has: dialogue text, learning points
  â†“
All content in detected language
```

### 4. **Explainer Generation**
```
Topic + question + language
  â†“
Explain Agent (LLM via Ollama)
  â†“
Generates: {title, summary, points, wikipedia_keyword}
  â†“
Wikipedia Service fetches image using keyword
  â†“
Returns: explainer with image_url
```

### 5. **Animation Scene Building**
```
Storyboard scenes + Explainer
  â†“
Animation Agent selects actions based on dialogue
  â†“
Generates 3D animation scenes with:
  - action (hello, thinking, jumping, etc.)
  - dialogue text
  - duration estimate
  â†“
Frontend renders with ScenePlayer (Three.js)
```

### 6. **Response Playback**
```
Animation scenes ready
  â†“
ScenePlayer renders 3D character
  â†“
For each scene:
  - Play animation
  - Add dialogue to chat
  - Speak text via Web Speech API
  â†“
User can stop/replay at any time
```

### 7. **Caching Layer**
```
Response cached by input text
  â†“
Next identical question: instant retrieval
  â†“
Cache includes: scenes, animations, explainer
```

## ğŸ“‹ System Components

### **Frontend** (`kidz-gpt-frontend/`)
- **React + TypeScript + Vite**: Modern SPA framework
- **Three.js + React Three Fiber**: 3D character rendering
- **Tailwind CSS**: Responsive styling
- **Radix UI**: Accessible component library
- **Web Speech API**: Browser TTS/STT

### **Backend** (`kidz-gpt-backend/`)

#### **Main App** (`app/`)
- `main.py`: FastAPI server with endpoints
- `orchestrator.py`: Pipeline orchestration & language handling

#### **Agents** (`agents/`)
- `intent_agent.py`: Extracts learning intent from user input
- `script_agent.py`: Generates educational storyboard
- `explain_agent.py`: Creates explanations with Wikipedia keywords
- `animation_agent.py`: Selects animations based on dialogue
- `quiz_agent.py`: Generates and scores quizzes

#### **Services** (`services/`)
- `stt_service.py`: Speech-to-text via Whisper
- `language_service.py`: Language detection with script-based fallback
- `wikipedia_service.py`: Fetches images from Wikipedia
- `cache_service.py`: Redis-based response caching
- `safety_service.py`: Content safety checking
- `animation_script_service.py`: Animation scene building
- `translation_service.py`: Text translation

#### **Whisper Server** (`whisper_server.py`)
- Dedicated FastAPI server for speech recognition
- Runs on port 8001
- Uses 'base' model for multilingual support

## ğŸš€ Installation & Setup

### **Prerequisites**
- Python 3.9+
- Node.js 18+
- FFmpeg (for Whisper)
- Ollama (local LLM server)
- Redis (optional, for caching)

### **Step 1: Clone & Install Python Dependencies**

```bash
cd KIDZ-GPT

# Backend setup
cd kidz-gpt-backend
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### **Step 2: Install Ollama**

1. Download from [ollama.ai](https://ollama.ai)
2. Run the installer
3. Pull the required model:
   ```bash
   ollama pull gpt-oss:120b-cloud
   ```
4. Start Ollama (runs on `http://localhost:11434` by default)

### **Step 3: Install FFmpeg**

**Windows:**
```bash
pip install imageio-ffmpeg
```

**macOS:**
```bash
brew install ffmpeg
```

**Linux:**
```bash
sudo apt-get install ffmpeg
```

### **Step 4: Set Up Environment Variables**

Create `.env` file in `kidz-gpt-backend/`:
```env
# Ollama Configuration
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=gpt-oss:120b-cloud
OLLAMA_MODEL_INTENT=gpt-oss:120b-cloud

# Whisper Configuration
WHISPER_MODEL=base

# Server Configuration
STT_TIMEOUT_SECONDS=180
CACHE_TTL_SECONDS=3600

# Redis (optional)
REDIS_URL=redis://localhost:6379/0

# Frontend
VITE_API_URL=http://localhost:8000
```

### **Step 5: Start Backend Services**

**Terminal 1 - Whisper Server:**
```bash
cd kidz-gpt-backend
python whisper_server.py
# Runs on http://localhost:8001
```

**Terminal 2 - Main Backend:**
```bash
cd kidz-gpt-backend
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
# Runs on http://localhost:8000
```

### **Step 6: Install & Start Frontend**

```bash
cd kidz-gpt-frontend

# Install dependencies
npm install

# Start development server
npm run dev:client
# Runs on http://localhost:5000
```

### **Step 7: Access the Application**

Open your browser and navigate to:
```
http://localhost:5000
```

## ğŸ¯ Usage

### **Voice Input**
1. Click the microphone button
2. Speak a question (e.g., "What is a pen?")
3. System automatically detects language
4. Animated character responds with explanation
5. Watch the quiz section appear for reinforcement

### **Text Input**
1. Click the text input field
2. Type your question
3. System auto-detects language from text
4. Get animated response with explanation
5. Take an interactive quiz

### **Language Selection**
- System automatically detects from audio/text
- Manual selection available in language dropdown
- Supports: English, Hindi, Bengali, Tamil, Telugu

### **Controls**
- **â¹ï¸ Stop**: Halt current response playback
- **ğŸ”„ Replay**: Re-play the last response
- **ğŸ§ Quiz**: Take a quiz on the topic
- **ğŸ–¼ï¸ Background**: Toggle between two themed backgrounds

## ğŸ§  How Language Detection Works

### **Priority Order:**
1. **Whisper Audio Detection** (Most Accurate)
   - OpenAI Whisper analyzes audio
   - Detects language from speech patterns
   - Example: User speaks in Hindi â†’ Whisper returns "hi"

2. **Text-Based Detection** (Fallback for text input)
   - `langdetect` library analyzes text
   - Script-based detection (Devanagari â†’ Hindi, etc.)
   - Example: Text contains à¤¹à¤¿à¤‚à¤¦à¥€ characters â†’ Detected as "hi"

3. **User-Specified Language**
   - Falls back if auto-detection fails
   - User can manually override

4. **Default to English**
   - Safest fallback option

### **Language Pipeline:**
```
Input (Audio/Text) 
  â†“
Detection (Whisper/langdetect)
  â†“
Normalization (hi-IN â†’ hi)
  â†“
Validation (Check if supported)
  â†“
Pipeline Execution (All agents respond in this language)
  â†“
TTS (Web Speech API uses detected language)
```

## ğŸ¨ UI/UX Features

### **Chat Bubbles**
- **User Messages**: Coral-to-Orange gradient (ğŸ¤ You)
- **AI Messages**: Green gradient (ğŸ¤– KidzGPT)
- **Listening**: Blue gradient (ğŸ§ Listening)
- **Thinking**: Golden gradient (âœ¨ Thinking)

### **Explainer Section**
- Topic title and image (from Wikipedia)
- Summary in child-friendly language
- 3 key learning points
- Topic tags

### **3D Animation**
- Two character options (Boy & Girl)
- Animated expressions (hello, thinking, surprised, etc.)
- Floating decorative elements
- Smooth transitions

## ğŸ”§ Configuration

### **Whisper Model Sizes**
- `tiny`: Fast, lower accuracy (â†’ use for quick tests)
- `small`: Good balance
- `base`: Better multilingual (â†’ default)
- `medium`: Higher accuracy (â†’ slower)

### **Ollama Model Options**
```bash
# Current (recommended)
ollama pull gpt-oss:120b-cloud

# Alternatives
ollama pull mistral
ollama pull neural-chat
ollama pull dolphin-mixtral
```

### **TTS Voice Selection**
- System automatically selects best voice for language
- Prioritizes neural voices
- Falls back to other voices if needed

## ğŸ“Š Supported Languages

| Language | Code | Script Detection |
|----------|------|------------------|
| English | en | Latin |
| Hindi | hi | Devanagari (à¤¹à¤¿à¤‚à¤¦à¥€) |
| Bengali | bn | Bengali (à¦¬à¦¾à¦‚à¦²à¦¾) |
| Tamil | ta | Tamil (à®¤à®®à®¿à®´à¯) |
| Telugu | te | Telugu (à°¤à±†à°²à±à°—à±) |

Each language has:
- âœ… Whisper STT support
- âœ… Intent/Script/Explain agents
- âœ… Web Speech API TTS
- âœ… Fallback explainers
- âœ… Quiz generation

## ğŸ› Troubleshooting

### **"Whisper server not responding"**
```bash
# Check if Whisper server is running on port 8001
curl http://localhost:8001/transcribe

# Restart if needed
python whisper_server.py
```

### **"Ollama connection failed"**
```bash
# Ensure Ollama is running
ollama serve

# Check if model is installed
ollama list
```

### **"Language not detected correctly"**
- Check backend logs for language detection flow
- Verify Whisper model is 'base' or higher
- For text input, ensure text contains language script markers

### **"No audio recorded"**
- Check browser microphone permissions
- Ensure HTTPS or localhost (browser requirement)
- Test microphone in another app

### **"Animation not playing"**
- Check Three.js/React Three Fiber in browser console
- Verify character model files exist
- Clear browser cache and reload

## ğŸ“ API Endpoints

### **Main Endpoints**

**POST** `/process`
- Audio processing with language hint
- Returns: Scenes, animation, explainer, quiz

**POST** `/process-text`
- Text input processing
- Returns: Same as /process

**POST** `/generate-quiz`
- Generate quiz for a topic
- Returns: Quiz questions with scoring

### **Whisper Endpoint**

**POST** `/transcribe` (Port 8001)
- Audio file + language hint
- Returns: {text, detected_language}

## ğŸš¦ Performance Tips

- **First Load**: ~3-5 seconds (Whisper model loading)
- **Response Time**: ~5-15 seconds (LLM generation)
- **Animation Playback**: Real-time
- **Caching**: Identical queries return instantly

## ğŸ“± Browser Support

- Chrome/Edge 90+
- Firefox 88+
- Safari 14+
- Opera 76+

**Requirements:**
- Web Speech API (TTS)
- Web Audio API (microphone access)
- WebGL (3D animation)

## ğŸ“ Educational Design

### **For Kids:**
- âœ¨ Engaging 3D characters
- ğŸ¨ Colorful, vibrant UI
- ğŸ¤ Voice interaction
- ğŸ§© Interactive quizzes
- ğŸŒ Multilingual support

### **Learning Approach:**
1. **Explanation**: AI provides structured learning
2. **Visualization**: Wikipedia images show real examples
3. **Reinforcement**: Interactive quiz tests understanding
4. **Feedback**: Instant scoring and encouragement

## ğŸ“š Project Structure

```
KIDZ-GPT/
â”œâ”€â”€ kidz-gpt-backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py (FastAPI endpoints)
â”‚   â”‚   â””â”€â”€ orchestrator.py (Pipeline)
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ intent_agent.py
â”‚   â”‚   â”œâ”€â”€ script_agent.py
â”‚   â”‚   â”œâ”€â”€ explain_agent.py
â”‚   â”‚   â”œâ”€â”€ animation_agent.py
â”‚   â”‚   â””â”€â”€ quiz_agent.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ stt_service.py
â”‚   â”‚   â”œâ”€â”€ language_service.py
â”‚   â”‚   â”œâ”€â”€ wikipedia_service.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ whisper_server.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ kidz-gpt-frontend/
    â”œâ”€â”€ client/
    â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”œâ”€â”€ pages/
    â”‚   â”‚   â”‚   â””â”€â”€ home.tsx (Main interface)
    â”‚   â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”‚   â”œâ”€â”€ ScenePlayer.tsx (3D animation)
    â”‚   â”‚   â”‚   â””â”€â”€ characters/
    â”‚   â”‚   â””â”€â”€ index.css
    â”‚   â””â”€â”€ public/
    â”‚       â””â”€â”€ assets/
    â”‚           â””â”€â”€ models/ (3D character models)
    â”œâ”€â”€ package.json
    â”œâ”€â”€ vite.config.ts
    â””â”€â”€ tsconfig.json
```

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional languages
- More 3D character models
- Enhanced quiz generation
- Performance optimization
- Mobile app version

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ‰ Acknowledgments

- OpenAI Whisper for speech recognition
- Ollama for local LLM inference
- Three.js for 3D rendering
- React Three Fiber for React integration
- Radix UI for accessible components
- Tailwind CSS for styling

---

**Built with â¤ï¸ for kids' learning**

For issues, questions, or suggestions, please open an issue on GitHub.
